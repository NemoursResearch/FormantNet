{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FormantNet Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for defining, training, or using FormantNet models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from FN_data import getdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling Model Output\n",
    "The **rescale_params()** function takes the output of the model, which is typically on a scale of 0 to 1 or -1 to 1, and rescales it to the expected scale for formant parameters (e.g. 0-8000 Hz for frequencies). The minimum and maximum values allowed for each formant parameter type (frequency, bandwidth, and amplitude) can be adjusted by the user using the configuration file. The input scale depends on the output activation function of the model (TOP_ACTIVATION); we experimented with linear, tanh, softsign, and ReLU, but found that sigmoid usually works best.  \n",
    "  \n",
    "Notes: \n",
    "* As defined by the use of the tf.split below, this function forces the output of the model to be in the order **F1 F2 F3 ... B1 B2 B3 ... A1 A2 A3 ....** (which may be subsequently rearranged by **track_files()**). \n",
    "* If ReLU is used as the activation function, the formant maximum values defined by cfg.MAXFREQ, cfg.MAXBW, and cfg.MAXAMP will be ignored. If linear is used, both minima and maxima are ignored.\n",
    "* This function is needed for evaluation and any future use of the model (see again  **track_files()** below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rescale_fn(cfg):\n",
    "    @tf.function(input_signature=(tf.TensorSpec(shape=[None, cfg.NPARAMS], dtype=tf.float32),))\n",
    "    def rescale_params(params):\n",
    "        freqs, bws, amps = tf.split(params, [cfg.NSUM, cfg.NSUM, cfg.NFORMANTS], axis=-1)\n",
    "        if cfg.TOP_ACTIVATION == 'sigmoid':    #network produces values between 0 and 1\n",
    "            freqs = cfg.MINFREQ + (freqs * (cfg.MAXFREQ - cfg.MINFREQ))\n",
    "            bws = cfg.MINBW + (bws * (cfg.MAXBW - cfg.MINBW))\n",
    "            amps = cfg.MINAMP + (amps * (cfg.MAXAMP - cfg.MINAMP))\n",
    "        elif cfg.TOP_ACTIVATION == 'softsign' or cfg.TOP_ACTIVATION == 'tanh':  #network produces values between -1 and 1\n",
    "            freqs = cfg.MINFREQ + ((freqs + 1.0) * ((cfg.MAXFREQ - cfg.MINFREQ) / 2.0))\n",
    "            bws = cfg.MINBW + ((bws + 1.0) * ((cfg.MAXBW - cfg.MINBW) / 2.0))\n",
    "            amps = cfg.MINAMP + ((amps + 1.0) * ((cfg.MAXAMP - cfg.MINAMP) / 2.0))\n",
    "        elif cfg.TOP_ACTIVATION == 'relu':   #network produces values of 0 or greater\n",
    "            freqs = freqs + cfg.MINFREQ\n",
    "            bws = bws + cfg.MINBW\n",
    "            amps = amps + cfg.MINAMP\n",
    "        return freqs, bws, amps\n",
    "    \n",
    "    return rescale_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The functions used to compute the loss are defined here. We tried to write the code so that it could handle variations in sampling rate (SAMPLERATE), frequency range (from 0 to MAX_ANALYSIS_FREQ), number of formants (NFORMANTS), number of anti-formants (NZEROS), spectral resolution (SPECTRUM_NPOINTS), and the activation type of the final model output layer (TOP_ACTIVATION). For the TIMIT experiments, these were all set constant across all experiments: 16K sampling rate, 0-8K frequency range, 6 formants, 1 zero, 257-point spectra, sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **formant()** function takes the frequency **F** and bandwidth **B** of each formant predicted by the model, and generates a corresponding formant: an array of spectrum levels **h** at each frequency bin **f** in the spectrum range at the given resolution (see Eqn. (1) of the IS2021 paper). The **vtfn()** function weights these by their corresponding amplitude factors, and combines them (multiplying or dividing, corresponding to whether it's a pole or zero) to produce a linear-scale spectral envelope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vtfn_func(cfg):\n",
    "    spec1 = tf.cast(np.linspace(0.0, cfg.MAX_ANALYSIS_FREQ, cfg.SPECTRUM_NPOINTS), dtype=tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def formant(freq, bw, nres):\n",
    "        fex = tf.expand_dims(freq, axis=-1)\n",
    "        bex = tf.expand_dims(bw, axis=-1)\n",
    "        bsq = bex**2 * 0.25\n",
    "        anum = fex**2 + bsq    \n",
    "        #spec1 = tf.cast(np.linspace(0.0, cfg.MAX_ANALYSIS_FREQ, cfg.SPECTRUM_NPOINTS), dtype=tf.float32)\n",
    "        spec2 = tf.tile(spec1, [tf.size(freq)])\n",
    "        spec = tf.reshape(spec2, [-1, nres, cfg.SPECTRUM_NPOINTS])\n",
    "        negspec = (spec - fex)**2 + bsq\n",
    "        posspec = (spec + fex)**2 + bsq\n",
    "        formants = anum / tf.math.sqrt(negspec * posspec)\n",
    "        return(formants)\n",
    "\n",
    "    #Note that vtfn returns a LINEAR-scale spectrum\n",
    "    if cfg.NZEROS == 0:\n",
    "        @tf.function\n",
    "        def vtfn(freqs, bws, amps):\n",
    "            ax = tf.expand_dims(amps, axis=-1)\n",
    "            ax = 10.0 ** (ax / 20.0)   #convert log amplitudes to linear\n",
    "            specs = formant(freqs, bws, cfg.NFORMANTS)\n",
    "            sumspec = tf.reduce_sum(ax * specs, axis = -2)\n",
    "            return sumspec\n",
    "    else:\n",
    "        @tf.function\n",
    "        def vtfn(freqs, bws, amps, zfreqs, zbws):\n",
    "            ax = tf.expand_dims(amps, axis=-1)\n",
    "            ax = 10.0 ** (ax / 20.0)   #convert log amplitudes to linear\n",
    "            fspecs = ax * formant(freqs, bws, cfg.NFORMANTS)\n",
    "            sumspecs = tf.reduce_sum(fspecs, axis = -2, keepdims=True)\n",
    "            zspecs = 1.0 / formant(zfreqs, zbws, cfg.NZEROS)\n",
    "            allspecs = tf.concat([sumspecs, zspecs], axis = -2)\n",
    "            prodspecs = tf.reduce_prod(allspecs, axis = -2)\n",
    "            return prodspecs\n",
    "        \n",
    "    return vtfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model loss is calculated with **custom_loss()**. First, the batch and sequence dimensions are collapsed. Then the input model parameters are rescaled with **rescale_params()**. The formants are split into poles and zeros, and sent to **vtfn()** to compute a linear-scale spectral envelope. The envelope is then converted to decibel scale, and the spectral loss is calculated as the mean square difference between the generated envelope and the original envelope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Delta-Frequency Loss (added loss equal to variation in predicted formant frequency over time) is being used, it is also calculated here, weighted by the **DIFFWEIGHT** parameter, and added to the spectral loss to get the final loss. (We also experimented with using delta-loss with bandwidths and amplitudes, but those experiments have been unsuccessful so far.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_loss(cfg, rescale_params, vtfn):\n",
    "    if cfg.NZEROS == 0:\n",
    "        @tf.function(input_signature=(tf.TensorSpec(shape=[None, None, cfg.SPECTRUM_NPOINTS], dtype=tf.float32),\n",
    "                                     tf.TensorSpec(shape=[None, None, cfg.NPARAMS], dtype=tf.float32)))\n",
    "        def custom_loss(specs_input, params_pred):\n",
    "            specs_input = tf.reshape(specs_input, [-1, cfg.SPECTRUM_NPOINTS])\n",
    "            params_pred = tf.reshape(params_pred, [-1, cfg.NPARAMS])\n",
    "            freqs, bws, amps = rescale_params(params_pred)\n",
    "            specs_pred = vtfn(freqs, bws, amps)\n",
    "            specs_pred = 20.0 * tf.math.log(cfg.FLOOR + specs_pred) / tf.math.log(10.0)\n",
    "            mse = tf.math.reduce_mean(tf.square(specs_input - specs_pred)) #loss over whole batch\n",
    "            fdiff = tf.math.reduce_mean(tf.abs(freqs[1:] - freqs[:-1]))\n",
    "            #bdiff = tf.math.reduce_mean(tf.abs(bws[1:] - bws[:-1]))\n",
    "            #adiff = tf.math.reduce_mean(tf.abs(amps[1:] - amps[:-1]))\n",
    "            #return (mse + cfg.DIFFWEIGHT * (fdiff + bdiff + 25.0 * adiff))\n",
    "            return (mse + cfg.DIFFWEIGHT * fdiff)\n",
    "\n",
    "    else:  \n",
    "        @tf.function(input_signature=(tf.TensorSpec(shape=[None, None, cfg.SPECTRUM_NPOINTS], dtype=tf.float32),\n",
    "                                     tf.TensorSpec(shape=[None, None, cfg.NPARAMS], dtype=tf.float32)))\n",
    "        def custom_loss(specs_input, params_pred):\n",
    "            specs_input = tf.reshape(specs_input, [-1, cfg.SPECTRUM_NPOINTS])\n",
    "            params_pred = tf.reshape(params_pred, [-1, cfg.NPARAMS])\n",
    "            freqs, bws, amps = rescale_params(params_pred)\n",
    "            pfreqs, zfreqs = tf.split(freqs, [cfg.NFORMANTS, cfg.NZEROS], axis=-1)\n",
    "            pbws, zbws = tf.split(bws, [cfg.NFORMANTS, cfg.NZEROS], axis=-1)\n",
    "            specs_pred = vtfn(pfreqs, pbws, amps, zfreqs, zbws)\n",
    "            specs_pred = 20.0 * tf.math.log(cfg.FLOOR + specs_pred) / tf.math.log(10.0)\n",
    "            mse = tf.math.reduce_mean(tf.square(specs_input - specs_pred)) #loss over whole batch\n",
    "            fdiff = tf.math.reduce_mean(tf.abs(freqs[1:] - freqs[:-1]))\n",
    "            #bdiff = tf.math.reduce_mean(tf.abs(bws[1:] - bws[:-1]))\n",
    "            #adiff = tf.math.reduce_mean(tf.abs(amps[1:] - amps[:-1]))\n",
    "            #return (mse + cfg.DIFFWEIGHT * (fdiff + bdiff + 25.0 * adiff))\n",
    "            return (mse + cfg.DIFFWEIGHT * fdiff)\n",
    "    \n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "**define_model()** defines the model's architecture, loss function, and optimizer before compiling it, then prints summary statistics. In our TIMIT experiments, we produced the best results with just a single LSTM layer of 512 units followed by a single Dense layer, but the user may experiment with adding more layers of either type, and changing their sizes and activation functions, via the provided configuration file. The LEARNING_RATE used by the Adam optimizer can also be adjusted (default 0.0001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(cfg):\n",
    "\n",
    "    #Definition of model architecture:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(None, cfg.SPECTRUM_NPOINTS)))\n",
    "    for i in range(cfg.LSTM_LAYERS):\n",
    "        model.add(tf.keras.layers.LSTM(cfg.LSTM_UNITS, return_sequences=True))\n",
    "    for i in range(cfg.DENSE_LAYERS - 1):\n",
    "        model.add(tf.keras.layers.Dense(cfg.DENSE_UNITS, activation=cfg.DENSE_ACTIVATION))\n",
    "    model.add(tf.keras.layers.Dense(cfg.NPARAMS, activation=cfg.TOP_ACTIVATION))\n",
    "\n",
    "    #Here the model's loss function is defined before the model is compiled.\n",
    "    rescale_params = get_rescale_fn(cfg)\n",
    "    myvtfn = get_vtfn_func(cfg)\n",
    "    myloss = get_custom_loss(cfg, rescale_params, myvtfn)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(cfg.LEARNING_RATE),\n",
    "        loss=myloss, metrics=[myloss]\n",
    "    )\n",
    "\n",
    "    #Summary statistics:\n",
    "    print(\"\\nModel input shape: ({}, {}, {})\".format(cfg.BATCH_SIZE, cfg.SEQUENCE_LENGTH, cfg.SPECTRUM_NPOINTS))\n",
    "    print(\"Model output shape: ({}, {}, {})\".format(cfg.BATCH_SIZE, cfg.SEQUENCE_LENGTH, cfg.NPARAMS))\n",
    "    model.summary()\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_model()** does the model training. During training, the model weights are saved after every epoch that produces a validation loss lower than that of any previous epoch. Models are trained until the best validation loss has not improved after cfg.PATIENCE epochs, or a maximum of cfg.EPOCHS epochs. If there is no validation set, the training set loss is used for monitoring instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ALLOW_RETRAIN is True and there is at least one pre-existing model in the model directory, the newest saved model will be reloaded, and training will pick up where it left off with that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-best models are deleted, unless DELETE_OLDER_MODELS is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, modeldir, cfg, train_dset, val_dset=None):\n",
    "\n",
    "    checkpoints = glob.glob(modeldir + \"/*.index\")\n",
    "    if len(checkpoints) > 0 and cfg.ALLOW_RETRAIN and not cfg.TESTRUN:\n",
    "        latest = tf.train.latest_checkpoint(modeldir)\n",
    "        model.load_weights(latest)\n",
    "        print(\"\\nReloading pre-existing model\", latest, \"for further training.\")\n",
    "        last_epoch = int(latest[-3:])\n",
    "    else:\n",
    "        last_epoch = 0\n",
    "\n",
    "    if val_dset is None:\n",
    "        mymonitor = 'custom_loss'\n",
    "    else:\n",
    "        mymonitor = 'val_custom_loss'\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=modeldir + \"/model.epoch{epoch:03d}\",\n",
    "        save_best_only=True, save_weights_only=True,\n",
    "        monitor=mymonitor, mode='min')\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=cfg.PATIENCE, monitor=mymonitor, mode='min')\n",
    "\n",
    "    if cfg.TESTRUN:\n",
    "        Verbosity = 1\n",
    "    else:\n",
    "        Verbosity = 2\n",
    "\n",
    "    print(\"\\nBegin training:\")\n",
    "    sys.stdout.flush()\n",
    "    model.fit(train_dset, epochs=cfg.EPOCHS, initial_epoch=last_epoch, \n",
    "              validation_data=val_dset, verbose=Verbosity,\n",
    "              callbacks=[model_checkpoint_callback, early_stopping_callback])\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if cfg.DELETE_OLDER_MODELS:\n",
    "        print(\"\\nDeleting non-best models....\")\n",
    "        latest = tf.train.latest_checkpoint(modeldir)\n",
    "        for i in glob.glob(modeldir + \"/model.epoch*\"):\n",
    "            if not i.startswith(latest):\n",
    "                #print(\"Deleting\", i)\n",
    "                os.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating formant tracks on test files\n",
    "\n",
    "The **track_files()** function runs a trained model on a list of wavefiles, generating formant measurements every cfg.FRAME_STRIDE_MSEC milliseconds (5 msec by default). These measurements are saved in text files, with one output line for every frame. The columns of the output file consist of the following in order: the filename, the time point in milliseconds, and then the total number of resonances (poles plus zeros). This is followed by the parameters (frequency, bandwidth, and amplitudes, in that order) of the poles, in order of increasing mean frequency, and then those of the zeros, in order of increasing mean absolute frequency. By default, the frequencies are listed first, then bandwidths, then amplitudes (i.e. F1 F2 F3 ... B1 B2 B3 ... etc.), but this can be switched to an interleaved ordering, i.e. F1 B1 A1, F2 B2 A2, etc., by setting the configuration parameter FREQUENCIES_FIRST to False.  \n",
    "  \n",
    "The names and locations of these output text files depend on whether an output directory name is provided. If so, Then all of the output text files are stored in this directory. To prevent name collisions, the output filename is derived from by converting all slashes in the input pathname to underscores -- but to shorten the names, any initial part of the input path that is common to all of the files is left out. As an illustration, the following example shows the corresponding output names for a list of 3 input files:\n",
    "\n",
    "timit/test/dr1/faks0/sa1.wav -> outdir/dr1_faks0_sa1.txt  \n",
    "timit/test/dr1/faks0/sa2.wav -> outdir/dr1_faks0_sa2.txt  \n",
    "timit/test/dr2/fcmr0/sa1.wav -> outdir/dr2_fcmr0_sa1.txt  \n",
    "\n",
    "On the other hand, if an output directory name is **not** provided, then each output file is written to the same directory as the corresponding input file, with the same name as the wavefile but a different extension (.txt).  \n",
    "\n",
    "In either case, the file extension can be changed via the configuration file (OUT_EXT).\n",
    "\n",
    "Other notes:\n",
    "* Since there is nothing in the custom loss code above that distinguishes one formant from another (aside from poles versus zeros), and any of them can take frequency values between MINFREQ and MAXFREQ, the model output neurons may generate the formants in any random order (although that order will be constant from one frame to the next; e.g. if neuron 3 generates F1 for one frame, it does so for all frames and files). track_files() reorders the formants by their mean frequencies over all frames.\n",
    "* track_files() changes the frequencies of the antiformants (zeros) to negative values, to distinguish them from the poles. Also, since the antiformants don't have their own amplitude correction factors, a placeholder value of \"1.0\" is inserted.\n",
    "* For output interpretation, it's important to remember that the initial \"amplitudes\" generated by the model are not actually final formant amplitudes, but rather weighting factors that are used to adjust the initial formant amplitudes generated by formant(). By default, track_files() attempts to convert these weighting factors to \"real\" formant amplitude estimates (but see README for more information).  This behavior can be changed to generate the original weighting factors by changing the configuration parameter REAL_AMPLITUDES to False.\n",
    "* Unlike the python scripts in IS2021/ and PaPE2021/, track_files() also performs the final binomial smoothing of the output parameters; the number of smoothing passes is controlled by BIN_SMOOTH_PASSES (default 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_files(testlist, model, trmean, trstd, cfg, outdir=None):\n",
    "\n",
    "    rescale_params = get_rescale_fn(cfg)\n",
    "\n",
    "    if outdir is not None:\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        #Determine how much of the directory path is common to all test files\n",
    "        y = [i.split(\"/\") for i in testlist]\n",
    "        n_common_prefixes = 0\n",
    "        b = 0\n",
    "        for i in y[0]:\n",
    "            for j in y[1:]:\n",
    "                if (j[n_common_prefixes] != i):\n",
    "                    b = 1\n",
    "                    break\n",
    "            if b == 1:\n",
    "                break\n",
    "            else:\n",
    "                n_common_prefixes += 1\n",
    "\n",
    "    print(\"\\nGenerating\", len(testlist), \"output files:\")\n",
    "    for filename in testlist:\n",
    "\n",
    "        #Determine output directories and filenames\n",
    "        if filename.endswith(\".wav\"):\n",
    "            shortname = filename.rsplit(\".\", 1)[0]\n",
    "        else:\n",
    "            shortname = filename\n",
    "        if outdir is None:\n",
    "            outname = shortname + '.' + cfg.OUT_EXT\n",
    "        else:\n",
    "            shortname = \"_\".join(shortname.split(\"/\")[n_common_prefixes:])\n",
    "            outname = outdir + \"/\" + shortname + '.' + cfg.OUT_EXT\n",
    "        print(filename, \" -> \", outname)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        #Note that we feed the files one at a time to getdata()\n",
    "        lenf, f1 = getdata([filename], cfg, verbose=0)\n",
    "        #Again, the input data must be normalized by the training set statistics\n",
    "        f2 = (f1 - trmean) / trstd\n",
    "        #Add a third (batch) dimension so that frame sequence can be read directly by model\n",
    "        f2 = tf.expand_dims(f2, axis=0)\n",
    "        #Generate formant predictions\n",
    "        y = model.predict(f2)[0]\n",
    "        # Rescale and reorganize output\n",
    "        f, b, a = rescale_params(y)\n",
    "        zf = f.numpy()\n",
    "        za = a.numpy()\n",
    "        zb = b.numpy()        \n",
    "\n",
    "        # Convert antiformant frequencies to negative numbers, and \n",
    "        # insert placeholder \"1.0\" for antiformant amplitudes.\n",
    "        # Then sort formants and antiformants separately by increasing frequency.\n",
    "        if cfg.NZEROS > 0:\n",
    "            fp, f0 = tf.split(zf, [cfg.NFORMANTS, cfg.NZEROS], axis=-1)\n",
    "            f0 = f0 * -1.0\n",
    "            zf = tf.concat([fp, f0], axis=-1)\n",
    "            a0 = tf.ones([za.shape[0], cfg.NZEROS], dtype=tf.float32)\n",
    "            za = tf.concat([za, a0], axis=-1)    \n",
    "            ord1 = np.hstack((np.argsort(np.mean(fp, axis=0)), \n",
    "                              (np.flip(np.argsort(np.mean(f0, axis=0)) + cfg.NFORMANTS))))\n",
    "        else:\n",
    "            ord1 = np.argsort(np.mean(np.abs(zf),axis=0))\n",
    "\n",
    "        # Convert amplitude adjustment factors to true estimates of amplitudes\n",
    "        if cfg.REAL_AMPLITUDES:\n",
    "            za2 = 10.0 ** (za / 20.0)\n",
    "            zf2 = zf ** 2.0\n",
    "            zb2 = (zb ** 2.0) / 4.0\n",
    "            num = za2 * (zf2 + zb2)\n",
    "            den = ((4.0 * zb2 * zf2) + (zb2 ** 2.0)) ** 0.5\n",
    "            rat = num / den\n",
    "            za = 20.0 * np.log10(cfg.FLOOR + rat)\n",
    "\n",
    "        # Sort parameters in the order F1 F2 F3 ... B1 B2 B3 ... A1 A2 A3 ...\n",
    "        if cfg.FREQUENCIES_FIRST:\n",
    "            ord2 = [i for i in ord1] + [i+cfg.NSUM for i in ord1] + [i+cfg.NSUM*2 for i in ord1]\n",
    "        # Otherwise output in order F1 B1 A1 F2 B2 A2 F3 B3 A3 ... \n",
    "        else:\n",
    "            p = [(i, i+cfg.NSUM, i+(cfg.NSUM*2)) for i in ord1]\n",
    "            ord2 = sum(p, ())\n",
    "\n",
    "        # Do binomial smoothing of output, if any\n",
    "        zp = np.hstack((zf, zb, za))\n",
    "        for i in range(cfg.BIN_SMOOTH_PASSES):\n",
    "            zp = np.vstack((0.75*zp[0] + 0.25*zp[1],\n",
    "                            0.5*zp[1:-1,] + 0.25*zp[2:,] + 0.25*zp[:-2,],\n",
    "                            0.75*zp[-1] + 0.25*zp[-2]))\n",
    "\n",
    "        # Write to output file\n",
    "        out1 = zp[:,ord2]\n",
    "        ff = open(outname, \"w\")\n",
    "        for i in range(out1.shape[0]):\n",
    "            ff.write(\"{} {:.1f} {}   \".format(shortname, i * cfg.FRAME_STRIDE_MSEC, cfg.NSUM))\n",
    "            out1[i,:].tofile(ff, sep=\" \", format=\"%.2f\")\n",
    "            ff.write(\" \\n\")\n",
    "        ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code bebugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import FN_configuration\n",
    "    cfg = FN_configuration.configuration()\n",
    "    cfg.configure(None)\n",
    "    model = define_model(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
